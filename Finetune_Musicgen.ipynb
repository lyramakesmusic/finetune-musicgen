{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ikoib4YboTBY",
        "yG68hiK5vxA2",
        "us8clCVnv1IY",
        "9zlPbcLjv4qx",
        "UjevANVKv_dw"
      ],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XelDA1GGvaXw"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/facebookresearch/audiocraft.git\n",
        "%cd audiocraft\n",
        "!pip install -e .\n",
        "!pip install dora-search\n",
        "!pip install numba"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚠️ read the whole notebook first! ⚠️\n",
        "There's high odds that your question is already answered somewhere in here. pay attention to details like file paths, and skim through the example code. make sure you understand it before diving face first into a training run!"
      ],
      "metadata": {
        "id": "Hajimhie5Plf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data preprocessing etc\n",
        "\n",
        "### --- this is optional if you've already got resampled 30s audio clips and .json labels ---\n",
        "\n",
        "This section includes a tool to slice your audio into 30s chunks, resample to 44100hz, and normalize. it also includes a WIP autolabeller based on essentia (https://essentia.upf.edu/models.html, https://colab.research.google.com/drive/1tFInmCYK2uX-PajYemERvtSojkT0vrhF)\n",
        "\n",
        "If you don't want to autolabel your data but you do want to split it, only run the first cell under this header. The rest is for essentia.\n",
        "\n",
        "Tags parsed by essentia:\n",
        "- genre\n",
        "- mood/theme\n",
        "- instrumentation\n",
        "- key, bpm (these are actually from librosa)\n",
        "\n",
        "This section is very WIP, and it's always better to label your own data. However, that can be a lot of work especially with a lot of samples. this should give you a decent baseline.\n",
        "\n",
        "> TODO: for the autolabeller to have best results, you provide an OpenAI API key when prompted so it can do some prompt enhancement on the tags essentia creates. Otherwise, it will train only on the raw tags. information TBA by GPT:\n",
        "> - natural language description\n",
        "> - enhanced tags (eg adding \"percussion\" if \"drums\" already exists)\n",
        "\n",
        "> TODO: add a song recognition API to fill in title and artist info\n",
        "\n",
        "> TODO: figure out vram requirements, and clean up to leave space for musicgen. for now, save the `.jsonl` files, restart the notebook (your split files should be safe in drive), run everything after this (replacing the `.jsonl` files with the ones you downloaded at the paths mentioned in the code), and you should be okay."
      ],
      "metadata": {
        "id": "ikoib4YboTBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# mount google drive to access dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/samples/train_musicgen_edm_uncond/train/outputs\""
      ],
      "metadata": {
        "id": "vZpD62WSvICH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c05cd5-3a69-4219-f85d-c4e4f7425533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split and resample\n",
        "# don't run this if your audio is already sliced and resampled\n",
        "\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "\n",
        "os.makedirs(os.path.join(dataset_path, \"original\"), exist_ok=True)\n",
        "\n",
        "for filename in os.listdir(dataset_path):\n",
        "    if filename.endswith(('.mp3', '.wav', '.flac')):\n",
        "\n",
        "        # move original file out of the way\n",
        "        os.rename(filename, f\"original/{filename}\")\n",
        "        audio = AudioSegment.from_file(f\"original/{filename}\")\n",
        "\n",
        "        # resample\n",
        "        audio = audio.set_frame_rate(44100)\n",
        "\n",
        "        # split into 30-second chunks\n",
        "        for i in range(0, len(audio), 30000):\n",
        "            chunk = audio[i:i+30000]\n",
        "            chunk.export(f\"{filename[:-4]}_chunk{i//1000}.wav\", format=\"wav\")\n"
      ],
      "metadata": {
        "id": "wuIIBXApr2Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install essentia and requirements\n",
        "\n",
        "# this line is needed for the tensorflow modules to install properly!!\n",
        "!sudo apt-get install build-essential libeigen3-dev libyaml-dev libfftw3-dev libtag1-dev libchromaprint-dev\n",
        "\n",
        "!pip install -U essentia-tensorflow"
      ],
      "metadata": {
        "id": "J0lknCOjoSAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download some essentia model weights\n",
        "\n",
        "!curl https://essentia.upf.edu/models/classification-heads/genre_discogs400/genre_discogs400-discogs-effnet-1.pb --output genre_discogs400-discogs-effnet-1.pb\n",
        "!curl https://essentia.upf.edu/models/feature-extractors/discogs-effnet/discogs-effnet-bs64-1.pb --output discogs-effnet-bs64-1.pb\n",
        "!curl https://essentia.upf.edu/models/classification-heads/mtg_jamendo_moodtheme/mtg_jamendo_moodtheme-discogs-effnet-1.pb --output mtg_jamendo_moodtheme-discogs-effnet-1.pb\n",
        "!curl https://essentia.upf.edu/models/classification-heads/mtg_jamendo_instrument/mtg_jamendo_instrument-discogs-effnet-1.pb --output mtg_jamendo_instrument-discogs-effnet-1.pb"
      ],
      "metadata": {
        "id": "Q7gZlO_rqnQd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a79af917-a4e5-412d-c1de-8953642ce3d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2009k  100 2009k    0     0   226k      0  0:00:08  0:00:08 --:--:--  358k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 17.5M  100 17.5M    0     0   715k      0  0:00:25  0:00:25 --:--:--  838k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2675k  100 2675k    0     0   459k      0  0:00:05  0:00:05 --:--:--  639k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2643k  100 2643k    0     0   507k      0  0:00:05  0:00:05 --:--:--  612k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title metadata (labels) for essentia - LONG CELL DONT OPEN\n",
        "genre_labels = [\n",
        "    \"Blues---Boogie Woogie\",\n",
        "    \"Blues---Chicago Blues\",\n",
        "    \"Blues---Country Blues\",\n",
        "    \"Blues---Delta Blues\",\n",
        "    \"Blues---Electric Blues\",\n",
        "    \"Blues---Harmonica Blues\",\n",
        "    \"Blues---Jump Blues\",\n",
        "    \"Blues---Louisiana Blues\",\n",
        "    \"Blues---Modern Electric Blues\",\n",
        "    \"Blues---Piano Blues\",\n",
        "    \"Blues---Rhythm & Blues\",\n",
        "    \"Blues---Texas Blues\",\n",
        "    \"Brass & Military---Brass Band\",\n",
        "    \"Brass & Military---Marches\",\n",
        "    \"Brass & Military---Military\",\n",
        "    \"Children's---Educational\",\n",
        "    \"Children's---Nursery Rhymes\",\n",
        "    \"Children's---Story\",\n",
        "    \"Classical---Baroque\",\n",
        "    \"Classical---Choral\",\n",
        "    \"Classical---Classical\",\n",
        "    \"Classical---Contemporary\",\n",
        "    \"Classical---Impressionist\",\n",
        "    \"Classical---Medieval\",\n",
        "    \"Classical---Modern\",\n",
        "    \"Classical---Neo-Classical\",\n",
        "    \"Classical---Neo-Romantic\",\n",
        "    \"Classical---Opera\",\n",
        "    \"Classical---Post-Modern\",\n",
        "    \"Classical---Renaissance\",\n",
        "    \"Classical---Romantic\",\n",
        "    \"Electronic---Abstract\",\n",
        "    \"Electronic---Acid\",\n",
        "    \"Electronic---Acid House\",\n",
        "    \"Electronic---Acid Jazz\",\n",
        "    \"Electronic---Ambient\",\n",
        "    \"Electronic---Bassline\",\n",
        "    \"Electronic---Beatdown\",\n",
        "    \"Electronic---Berlin-School\",\n",
        "    \"Electronic---Big Beat\",\n",
        "    \"Electronic---Bleep\",\n",
        "    \"Electronic---Breakbeat\",\n",
        "    \"Electronic---Breakcore\",\n",
        "    \"Electronic---Breaks\",\n",
        "    \"Electronic---Broken Beat\",\n",
        "    \"Electronic---Chillwave\",\n",
        "    \"Electronic---Chiptune\",\n",
        "    \"Electronic---Dance-pop\",\n",
        "    \"Electronic---Dark Ambient\",\n",
        "    \"Electronic---Darkwave\",\n",
        "    \"Electronic---Deep House\",\n",
        "    \"Electronic---Deep Techno\",\n",
        "    \"Electronic---Disco\",\n",
        "    \"Electronic---Disco Polo\",\n",
        "    \"Electronic---Donk\",\n",
        "    \"Electronic---Downtempo\",\n",
        "    \"Electronic---Drone\",\n",
        "    \"Electronic---Drum n Bass\",\n",
        "    \"Electronic---Dub\",\n",
        "    \"Electronic---Dub Techno\",\n",
        "    \"Electronic---Dubstep\",\n",
        "    \"Electronic---Dungeon Synth\",\n",
        "    \"Electronic---EBM\",\n",
        "    \"Electronic---Electro\",\n",
        "    \"Electronic---Electro House\",\n",
        "    \"Electronic---Electroclash\",\n",
        "    \"Electronic---Euro House\",\n",
        "    \"Electronic---Euro-Disco\",\n",
        "    \"Electronic---Eurobeat\",\n",
        "    \"Electronic---Eurodance\",\n",
        "    \"Electronic---Experimental\",\n",
        "    \"Electronic---Freestyle\",\n",
        "    \"Electronic---Future Jazz\",\n",
        "    \"Electronic---Gabber\",\n",
        "    \"Electronic---Garage House\",\n",
        "    \"Electronic---Ghetto\",\n",
        "    \"Electronic---Ghetto House\",\n",
        "    \"Electronic---Glitch\",\n",
        "    \"Electronic---Goa Trance\",\n",
        "    \"Electronic---Grime\",\n",
        "    \"Electronic---Halftime\",\n",
        "    \"Electronic---Hands Up\",\n",
        "    \"Electronic---Happy Hardcore\",\n",
        "    \"Electronic---Hard House\",\n",
        "    \"Electronic---Hard Techno\",\n",
        "    \"Electronic---Hard Trance\",\n",
        "    \"Electronic---Hardcore\",\n",
        "    \"Electronic---Hardstyle\",\n",
        "    \"Electronic---Hi NRG\",\n",
        "    \"Electronic---Hip Hop\",\n",
        "    \"Electronic---Hip-House\",\n",
        "    \"Electronic---House\",\n",
        "    \"Electronic---IDM\",\n",
        "    \"Electronic---Illbient\",\n",
        "    \"Electronic---Industrial\",\n",
        "    \"Electronic---Italo House\",\n",
        "    \"Electronic---Italo-Disco\",\n",
        "    \"Electronic---Italodance\",\n",
        "    \"Electronic---Jazzdance\",\n",
        "    \"Electronic---Juke\",\n",
        "    \"Electronic---Jumpstyle\",\n",
        "    \"Electronic---Jungle\",\n",
        "    \"Electronic---Latin\",\n",
        "    \"Electronic---Leftfield\",\n",
        "    \"Electronic---Makina\",\n",
        "    \"Electronic---Minimal\",\n",
        "    \"Electronic---Minimal Techno\",\n",
        "    \"Electronic---Modern Classical\",\n",
        "    \"Electronic---Musique Concrète\",\n",
        "    \"Electronic---Neofolk\",\n",
        "    \"Electronic---New Age\",\n",
        "    \"Electronic---New Beat\",\n",
        "    \"Electronic---New Wave\",\n",
        "    \"Electronic---Noise\",\n",
        "    \"Electronic---Nu-Disco\",\n",
        "    \"Electronic---Power Electronics\",\n",
        "    \"Electronic---Progressive Breaks\",\n",
        "    \"Electronic---Progressive House\",\n",
        "    \"Electronic---Progressive Trance\",\n",
        "    \"Electronic---Psy-Trance\",\n",
        "    \"Electronic---Rhythmic Noise\",\n",
        "    \"Electronic---Schranz\",\n",
        "    \"Electronic---Sound Collage\",\n",
        "    \"Electronic---Speed Garage\",\n",
        "    \"Electronic---Speedcore\",\n",
        "    \"Electronic---Synth-pop\",\n",
        "    \"Electronic---Synthwave\",\n",
        "    \"Electronic---Tech House\",\n",
        "    \"Electronic---Tech Trance\",\n",
        "    \"Electronic---Techno\",\n",
        "    \"Electronic---Trance\",\n",
        "    \"Electronic---Tribal\",\n",
        "    \"Electronic---Tribal House\",\n",
        "    \"Electronic---Trip Hop\",\n",
        "    \"Electronic---Tropical House\",\n",
        "    \"Electronic---UK Garage\",\n",
        "    \"Electronic---Vaporwave\",\n",
        "    \"Folk, World, & Country---African\",\n",
        "    \"Folk, World, & Country---Bluegrass\",\n",
        "    \"Folk, World, & Country---Cajun\",\n",
        "    \"Folk, World, & Country---Canzone Napoletana\",\n",
        "    \"Folk, World, & Country---Catalan Music\",\n",
        "    \"Folk, World, & Country---Celtic\",\n",
        "    \"Folk, World, & Country---Country\",\n",
        "    \"Folk, World, & Country---Fado\",\n",
        "    \"Folk, World, & Country---Flamenco\",\n",
        "    \"Folk, World, & Country---Folk\",\n",
        "    \"Folk, World, & Country---Gospel\",\n",
        "    \"Folk, World, & Country---Highlife\",\n",
        "    \"Folk, World, & Country---Hillbilly\",\n",
        "    \"Folk, World, & Country---Hindustani\",\n",
        "    \"Folk, World, & Country---Honky Tonk\",\n",
        "    \"Folk, World, & Country---Indian Classical\",\n",
        "    \"Folk, World, & Country---Laïkó\",\n",
        "    \"Folk, World, & Country---Nordic\",\n",
        "    \"Folk, World, & Country---Pacific\",\n",
        "    \"Folk, World, & Country---Polka\",\n",
        "    \"Folk, World, & Country---Raï\",\n",
        "    \"Folk, World, & Country---Romani\",\n",
        "    \"Folk, World, & Country---Soukous\",\n",
        "    \"Folk, World, & Country---Séga\",\n",
        "    \"Folk, World, & Country---Volksmusik\",\n",
        "    \"Folk, World, & Country---Zouk\",\n",
        "    \"Folk, World, & Country---Éntekhno\",\n",
        "    \"Funk / Soul---Afrobeat\",\n",
        "    \"Funk / Soul---Boogie\",\n",
        "    \"Funk / Soul---Contemporary R&B\",\n",
        "    \"Funk / Soul---Disco\",\n",
        "    \"Funk / Soul---Free Funk\",\n",
        "    \"Funk / Soul---Funk\",\n",
        "    \"Funk / Soul---Gospel\",\n",
        "    \"Funk / Soul---Neo Soul\",\n",
        "    \"Funk / Soul---New Jack Swing\",\n",
        "    \"Funk / Soul---P.Funk\",\n",
        "    \"Funk / Soul---Psychedelic\",\n",
        "    \"Funk / Soul---Rhythm & Blues\",\n",
        "    \"Funk / Soul---Soul\",\n",
        "    \"Funk / Soul---Swingbeat\",\n",
        "    \"Funk / Soul---UK Street Soul\",\n",
        "    \"Hip Hop---Bass Music\",\n",
        "    \"Hip Hop---Boom Bap\",\n",
        "    \"Hip Hop---Bounce\",\n",
        "    \"Hip Hop---Britcore\",\n",
        "    \"Hip Hop---Cloud Rap\",\n",
        "    \"Hip Hop---Conscious\",\n",
        "    \"Hip Hop---Crunk\",\n",
        "    \"Hip Hop---Cut-up/DJ\",\n",
        "    \"Hip Hop---DJ Battle Tool\",\n",
        "    \"Hip Hop---Electro\",\n",
        "    \"Hip Hop---G-Funk\",\n",
        "    \"Hip Hop---Gangsta\",\n",
        "    \"Hip Hop---Grime\",\n",
        "    \"Hip Hop---Hardcore Hip-Hop\",\n",
        "    \"Hip Hop---Horrorcore\",\n",
        "    \"Hip Hop---Instrumental\",\n",
        "    \"Hip Hop---Jazzy Hip-Hop\",\n",
        "    \"Hip Hop---Miami Bass\",\n",
        "    \"Hip Hop---Pop Rap\",\n",
        "    \"Hip Hop---Ragga HipHop\",\n",
        "    \"Hip Hop---RnB/Swing\",\n",
        "    \"Hip Hop---Screw\",\n",
        "    \"Hip Hop---Thug Rap\",\n",
        "    \"Hip Hop---Trap\",\n",
        "    \"Hip Hop---Trip Hop\",\n",
        "    \"Hip Hop---Turntablism\",\n",
        "    \"Jazz---Afro-Cuban Jazz\",\n",
        "    \"Jazz---Afrobeat\",\n",
        "    \"Jazz---Avant-garde Jazz\",\n",
        "    \"Jazz---Big Band\",\n",
        "    \"Jazz---Bop\",\n",
        "    \"Jazz---Bossa Nova\",\n",
        "    \"Jazz---Contemporary Jazz\",\n",
        "    \"Jazz---Cool Jazz\",\n",
        "    \"Jazz---Dixieland\",\n",
        "    \"Jazz---Easy Listening\",\n",
        "    \"Jazz---Free Improvisation\",\n",
        "    \"Jazz---Free Jazz\",\n",
        "    \"Jazz---Fusion\",\n",
        "    \"Jazz---Gypsy Jazz\",\n",
        "    \"Jazz---Hard Bop\",\n",
        "    \"Jazz---Jazz-Funk\",\n",
        "    \"Jazz---Jazz-Rock\",\n",
        "    \"Jazz---Latin Jazz\",\n",
        "    \"Jazz---Modal\",\n",
        "    \"Jazz---Post Bop\",\n",
        "    \"Jazz---Ragtime\",\n",
        "    \"Jazz---Smooth Jazz\",\n",
        "    \"Jazz---Soul-Jazz\",\n",
        "    \"Jazz---Space-Age\",\n",
        "    \"Jazz---Swing\",\n",
        "    \"Latin---Afro-Cuban\",\n",
        "    \"Latin---Baião\",\n",
        "    \"Latin---Batucada\",\n",
        "    \"Latin---Beguine\",\n",
        "    \"Latin---Bolero\",\n",
        "    \"Latin---Boogaloo\",\n",
        "    \"Latin---Bossanova\",\n",
        "    \"Latin---Cha-Cha\",\n",
        "    \"Latin---Charanga\",\n",
        "    \"Latin---Compas\",\n",
        "    \"Latin---Cubano\",\n",
        "    \"Latin---Cumbia\",\n",
        "    \"Latin---Descarga\",\n",
        "    \"Latin---Forró\",\n",
        "    \"Latin---Guaguancó\",\n",
        "    \"Latin---Guajira\",\n",
        "    \"Latin---Guaracha\",\n",
        "    \"Latin---MPB\",\n",
        "    \"Latin---Mambo\",\n",
        "    \"Latin---Mariachi\",\n",
        "    \"Latin---Merengue\",\n",
        "    \"Latin---Norteño\",\n",
        "    \"Latin---Nueva Cancion\",\n",
        "    \"Latin---Pachanga\",\n",
        "    \"Latin---Porro\",\n",
        "    \"Latin---Ranchera\",\n",
        "    \"Latin---Reggaeton\",\n",
        "    \"Latin---Rumba\",\n",
        "    \"Latin---Salsa\",\n",
        "    \"Latin---Samba\",\n",
        "    \"Latin---Son\",\n",
        "    \"Latin---Son Montuno\",\n",
        "    \"Latin---Tango\",\n",
        "    \"Latin---Tejano\",\n",
        "    \"Latin---Vallenato\",\n",
        "    \"Non-Music---Audiobook\",\n",
        "    \"Non-Music---Comedy\",\n",
        "    \"Non-Music---Dialogue\",\n",
        "    \"Non-Music---Education\",\n",
        "    \"Non-Music---Field Recording\",\n",
        "    \"Non-Music---Interview\",\n",
        "    \"Non-Music---Monolog\",\n",
        "    \"Non-Music---Poetry\",\n",
        "    \"Non-Music---Political\",\n",
        "    \"Non-Music---Promotional\",\n",
        "    \"Non-Music---Radioplay\",\n",
        "    \"Non-Music---Religious\",\n",
        "    \"Non-Music---Spoken Word\",\n",
        "    \"Pop---Ballad\",\n",
        "    \"Pop---Bollywood\",\n",
        "    \"Pop---Bubblegum\",\n",
        "    \"Pop---Chanson\",\n",
        "    \"Pop---City Pop\",\n",
        "    \"Pop---Europop\",\n",
        "    \"Pop---Indie Pop\",\n",
        "    \"Pop---J-pop\",\n",
        "    \"Pop---K-pop\",\n",
        "    \"Pop---Kayōkyoku\",\n",
        "    \"Pop---Light Music\",\n",
        "    \"Pop---Music Hall\",\n",
        "    \"Pop---Novelty\",\n",
        "    \"Pop---Parody\",\n",
        "    \"Pop---Schlager\",\n",
        "    \"Pop---Vocal\",\n",
        "    \"Reggae---Calypso\",\n",
        "    \"Reggae---Dancehall\",\n",
        "    \"Reggae---Dub\",\n",
        "    \"Reggae---Lovers Rock\",\n",
        "    \"Reggae---Ragga\",\n",
        "    \"Reggae---Reggae\",\n",
        "    \"Reggae---Reggae-Pop\",\n",
        "    \"Reggae---Rocksteady\",\n",
        "    \"Reggae---Roots Reggae\",\n",
        "    \"Reggae---Ska\",\n",
        "    \"Reggae---Soca\",\n",
        "    \"Rock---AOR\",\n",
        "    \"Rock---Acid Rock\",\n",
        "    \"Rock---Acoustic\",\n",
        "    \"Rock---Alternative Rock\",\n",
        "    \"Rock---Arena Rock\",\n",
        "    \"Rock---Art Rock\",\n",
        "    \"Rock---Atmospheric Black Metal\",\n",
        "    \"Rock---Avantgarde\",\n",
        "    \"Rock---Beat\",\n",
        "    \"Rock---Black Metal\",\n",
        "    \"Rock---Blues Rock\",\n",
        "    \"Rock---Brit Pop\",\n",
        "    \"Rock---Classic Rock\",\n",
        "    \"Rock---Coldwave\",\n",
        "    \"Rock---Country Rock\",\n",
        "    \"Rock---Crust\",\n",
        "    \"Rock---Death Metal\",\n",
        "    \"Rock---Deathcore\",\n",
        "    \"Rock---Deathrock\",\n",
        "    \"Rock---Depressive Black Metal\",\n",
        "    \"Rock---Doo Wop\",\n",
        "    \"Rock---Doom Metal\",\n",
        "    \"Rock---Dream Pop\",\n",
        "    \"Rock---Emo\",\n",
        "    \"Rock---Ethereal\",\n",
        "    \"Rock---Experimental\",\n",
        "    \"Rock---Folk Metal\",\n",
        "    \"Rock---Folk Rock\",\n",
        "    \"Rock---Funeral Doom Metal\",\n",
        "    \"Rock---Funk Metal\",\n",
        "    \"Rock---Garage Rock\",\n",
        "    \"Rock---Glam\",\n",
        "    \"Rock---Goregrind\",\n",
        "    \"Rock---Goth Rock\",\n",
        "    \"Rock---Gothic Metal\",\n",
        "    \"Rock---Grindcore\",\n",
        "    \"Rock---Grunge\",\n",
        "    \"Rock---Hard Rock\",\n",
        "    \"Rock---Hardcore\",\n",
        "    \"Rock---Heavy Metal\",\n",
        "    \"Rock---Indie Rock\",\n",
        "    \"Rock---Industrial\",\n",
        "    \"Rock---Krautrock\",\n",
        "    \"Rock---Lo-Fi\",\n",
        "    \"Rock---Lounge\",\n",
        "    \"Rock---Math Rock\",\n",
        "    \"Rock---Melodic Death Metal\",\n",
        "    \"Rock---Melodic Hardcore\",\n",
        "    \"Rock---Metalcore\",\n",
        "    \"Rock---Mod\",\n",
        "    \"Rock---Neofolk\",\n",
        "    \"Rock---New Wave\",\n",
        "    \"Rock---No Wave\",\n",
        "    \"Rock---Noise\",\n",
        "    \"Rock---Noisecore\",\n",
        "    \"Rock---Nu Metal\",\n",
        "    \"Rock---Oi\",\n",
        "    \"Rock---Parody\",\n",
        "    \"Rock---Pop Punk\",\n",
        "    \"Rock---Pop Rock\",\n",
        "    \"Rock---Pornogrind\",\n",
        "    \"Rock---Post Rock\",\n",
        "    \"Rock---Post-Hardcore\",\n",
        "    \"Rock---Post-Metal\",\n",
        "    \"Rock---Post-Punk\",\n",
        "    \"Rock---Power Metal\",\n",
        "    \"Rock---Power Pop\",\n",
        "    \"Rock---Power Violence\",\n",
        "    \"Rock---Prog Rock\",\n",
        "    \"Rock---Progressive Metal\",\n",
        "    \"Rock---Psychedelic Rock\",\n",
        "    \"Rock---Psychobilly\",\n",
        "    \"Rock---Pub Rock\",\n",
        "    \"Rock---Punk\",\n",
        "    \"Rock---Rock & Roll\",\n",
        "    \"Rock---Rockabilly\",\n",
        "    \"Rock---Shoegaze\",\n",
        "    \"Rock---Ska\",\n",
        "    \"Rock---Sludge Metal\",\n",
        "    \"Rock---Soft Rock\",\n",
        "    \"Rock---Southern Rock\",\n",
        "    \"Rock---Space Rock\",\n",
        "    \"Rock---Speed Metal\",\n",
        "    \"Rock---Stoner Rock\",\n",
        "    \"Rock---Surf\",\n",
        "    \"Rock---Symphonic Rock\",\n",
        "    \"Rock---Technical Death Metal\",\n",
        "    \"Rock---Thrash\",\n",
        "    \"Rock---Twist\",\n",
        "    \"Rock---Viking Metal\",\n",
        "    \"Rock---Yé-Yé\",\n",
        "    \"Stage & Screen---Musical\",\n",
        "    \"Stage & Screen---Score\",\n",
        "    \"Stage & Screen---Soundtrack\",\n",
        "    \"Stage & Screen---Theme\",\n",
        "]\n",
        "mood_theme_classes = [\n",
        "    \"action\",\n",
        "    \"adventure\",\n",
        "    \"advertising\",\n",
        "    \"background\",\n",
        "    \"ballad\",\n",
        "    \"calm\",\n",
        "    \"children\",\n",
        "    \"christmas\",\n",
        "    \"commercial\",\n",
        "    \"cool\",\n",
        "    \"corporate\",\n",
        "    \"dark\",\n",
        "    \"deep\",\n",
        "    \"documentary\",\n",
        "    \"drama\",\n",
        "    \"dramatic\",\n",
        "    \"dream\",\n",
        "    \"emotional\",\n",
        "    \"energetic\",\n",
        "    \"epic\",\n",
        "    \"fast\",\n",
        "    \"film\",\n",
        "    \"fun\",\n",
        "    \"funny\",\n",
        "    \"game\",\n",
        "    \"groovy\",\n",
        "    \"happy\",\n",
        "    \"heavy\",\n",
        "    \"holiday\",\n",
        "    \"hopeful\",\n",
        "    \"inspiring\",\n",
        "    \"love\",\n",
        "    \"meditative\",\n",
        "    \"melancholic\",\n",
        "    \"melodic\",\n",
        "    \"motivational\",\n",
        "    \"movie\",\n",
        "    \"nature\",\n",
        "    \"party\",\n",
        "    \"positive\",\n",
        "    \"powerful\",\n",
        "    \"relaxing\",\n",
        "    \"retro\",\n",
        "    \"romantic\",\n",
        "    \"sad\",\n",
        "    \"sexy\",\n",
        "    \"slow\",\n",
        "    \"soft\",\n",
        "    \"soundscape\",\n",
        "    \"space\",\n",
        "    \"sport\",\n",
        "    \"summer\",\n",
        "    \"trailer\",\n",
        "    \"travel\",\n",
        "    \"upbeat\",\n",
        "    \"uplifting\"\n",
        "]\n",
        "instrument_classes = [\n",
        "    \"accordion\",\n",
        "    \"acousticbassguitar\",\n",
        "    \"acousticguitar\",\n",
        "    \"bass\",\n",
        "    \"beat\",\n",
        "    \"bell\",\n",
        "    \"bongo\",\n",
        "    \"brass\",\n",
        "    \"cello\",\n",
        "    \"clarinet\",\n",
        "    \"classicalguitar\",\n",
        "    \"computer\",\n",
        "    \"doublebass\",\n",
        "    \"drummachine\",\n",
        "    \"drums\",\n",
        "    \"electricguitar\",\n",
        "    \"electricpiano\",\n",
        "    \"flute\",\n",
        "    \"guitar\",\n",
        "    \"harmonica\",\n",
        "    \"harp\",\n",
        "    \"horn\",\n",
        "    \"keyboard\",\n",
        "    \"oboe\",\n",
        "    \"orchestra\",\n",
        "    \"organ\",\n",
        "    \"pad\",\n",
        "    \"percussion\",\n",
        "    \"piano\",\n",
        "    \"pipeorgan\",\n",
        "    \"rhodes\",\n",
        "    \"sampler\",\n",
        "    \"saxophone\",\n",
        "    \"strings\",\n",
        "    \"synthesizer\",\n",
        "    \"trombone\",\n",
        "    \"trumpet\",\n",
        "    \"viola\",\n",
        "    \"violin\",\n",
        "    \"voice\"\n",
        "]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1JX_9JaIquIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from essentia.standard import MonoLoader, TensorflowPredictEffnetDiscogs, TensorflowPredict2D\n",
        "import numpy as np\n",
        "\n",
        "def filter_predictions(predictions, class_list, threshold=0.1):\n",
        "    predictions_mean = np.mean(predictions, axis=0)\n",
        "    sorted_indices = np.argsort(predictions_mean)[::-1]\n",
        "    filtered_indices = [i for i in sorted_indices if predictions_mean[i] > threshold]\n",
        "    filtered_labels = [class_list[i] for i in filtered_indices]\n",
        "    filtered_values = [predictions_mean[i] for i in filtered_indices]\n",
        "    return filtered_labels, filtered_values\n",
        "\n",
        "def make_comma_separated_unique(tags):\n",
        "    seen_tags = set()\n",
        "    result = []\n",
        "    for tag in ', '.join(tags).split(', '):\n",
        "        if tag not in seen_tags:\n",
        "            result.append(tag)\n",
        "            seen_tags.add(tag)\n",
        "    return ', '.join(result)\n",
        "\n",
        "def get_audio_features(audio_filename):\n",
        "    audio = MonoLoader(filename=audio_filename, sampleRate=16000, resampleQuality=4)()\n",
        "    embedding_model = TensorflowPredictEffnetDiscogs(graphFilename=\"discogs-effnet-bs64-1.pb\", output=\"PartitionedCall:1\")\n",
        "    embeddings = embedding_model(audio)\n",
        "\n",
        "    result_dict = {}\n",
        "\n",
        "    # predict genres\n",
        "    genre_model = TensorflowPredict2D(graphFilename=\"genre_discogs400-discogs-effnet-1.pb\", input=\"serving_default_model_Placeholder\", output=\"PartitionedCall:0\")\n",
        "    predictions = genre_model(embeddings)\n",
        "    filtered_labels, _ = filter_predictions(predictions, genre_labels)\n",
        "    filtered_labels = ', '.join(filtered_labels).replace(\"---\", \", \").split(', ')\n",
        "    result_dict['genres'] = make_comma_separated_unique(filtered_labels)\n",
        "\n",
        "    # predict mood/theme\n",
        "    mood_model = TensorflowPredict2D(graphFilename=\"mtg_jamendo_moodtheme-discogs-effnet-1.pb\")\n",
        "    predictions = mood_model(embeddings)\n",
        "    filtered_labels, _ = filter_predictions(predictions, mood_theme_classes, threshold=0.05)\n",
        "    result_dict['moods'] = make_comma_separated_unique(filtered_labels)\n",
        "\n",
        "    # predict instruments\n",
        "    instrument_model = TensorflowPredict2D(graphFilename=\"mtg_jamendo_instrument-discogs-effnet-1.pb\")\n",
        "    predictions = instrument_model(embeddings)\n",
        "    filtered_labels, _ = filter_predictions(predictions, instrument_classes)\n",
        "    result_dict['instruments'] = filtered_labels\n",
        "\n",
        "    return result_dict"
      ],
      "metadata": {
        "id": "MT2zxVBYrBQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create .jsonl from the extracted features, make a train/test split, and save in the right place.\n",
        "\n",
        "# set the following variable to True if you want to see a progress bar instead of the printed results:\n",
        "use_tqdm = False\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import librosa\n",
        "from pydub import AudioSegment\n",
        "import wave\n",
        "import re\n",
        "\n",
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "tqdm = partial(tqdm, position=0, leave=True)\n",
        "\n",
        "# make sure the .jsonl has a place to go\n",
        "os.makedirs(\"/content/audiocraft/egs/train\", exist_ok=True)\n",
        "os.makedirs(\"/content/audiocraft/egs/eval\", exist_ok=True)\n",
        "\n",
        "train_len = 0\n",
        "eval_len = 0\n",
        "\n",
        "with open(\"/content/audiocraft/egs/train/data.jsonl\", \"w\") as train_file, \\\n",
        "     open(\"/content/audiocraft/egs/eval/data.jsonl\", \"w\") as eval_file:\n",
        "\n",
        "    dset = tqdm(os.listdir(dataset_path)) if use_tqdm else os.listdir(dataset_path)\n",
        "    random.shuffle(dset)\n",
        "\n",
        "    for filename in dset:\n",
        "        result = get_audio_features(os.path.join(dataset_path, filename))\n",
        "\n",
        "        # TODO: make openai call, populate description and keywords\n",
        "\n",
        "        # get key and BPM\n",
        "        y, sr = librosa.load(os.path.join(dataset_path, filename))\n",
        "        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
        "        tempo = round(tempo) # not usually accurate lol\n",
        "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "        key = np.argmax(np.sum(chroma, axis=1))\n",
        "        key = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'][key]\n",
        "        length = librosa.get_duration(y=y, sr=sr)\n",
        "        # print(f\"{filename}: {result}, detected key {key}, detected bpm {tempo}\")\n",
        "\n",
        "        # THIS IS FOR MY OWN DATASET FORMAT\n",
        "        # Meant strictly to extract from format: \"artist name 4_chunk25.wav\"\n",
        "        # Modify for your own use!!\n",
        "        def extract_artist_from_filename(filename):\n",
        "            match = re.search(r'(.+?)\\s\\d+_chunk\\d+\\.wav', filename)\n",
        "            artist = match.group(1) if match else \"\"\n",
        "            return artist.replace(\"mix\", \"\").strip() if \"mix\" in artist else artist\n",
        "        artist_name = extract_artist_from_filename(filename)\n",
        "\n",
        "        # populate json\n",
        "        entry = {\n",
        "            \"key\": f\"{key}\",\n",
        "            \"artist\": artist_name,\n",
        "            \"sample_rate\": 44100,\n",
        "            \"file_extension\": \"wav\",\n",
        "            \"description\": \"\",\n",
        "            \"keywords\": \"\",\n",
        "            \"duration\": length,\n",
        "            \"bpm\": tempo,\n",
        "            \"genre\": result.get('genres', \"\"),\n",
        "            \"title\": \"\",\n",
        "            \"name\": \"\",\n",
        "            \"instrument\": result.get('instruments', \"\"),\n",
        "            \"moods\": result.get('moods', []),\n",
        "            \"path\": os.path.join(dataset_path, filename),\n",
        "        }\n",
        "        print(entry)\n",
        "\n",
        "        # train/test split\n",
        "        if random.random() < 0.85:\n",
        "            train_len += 1\n",
        "            train_file.write(json.dumps(entry) + '\\n')\n",
        "        else:\n",
        "            eval_len += 1\n",
        "            eval_file.write(json.dumps(entry) + '\\n')\n",
        "\n",
        "print(train_len)\n",
        "print(eval_len)\n",
        "\n",
        "# clear cuda mem for finetuning\n",
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ],
      "metadata": {
        "id": "2yN_ar3rrm86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load dataset into musicgen\n",
        "\n",
        "a dataset for musicgen is:\n",
        "- a .yaml file with basic info about the audio sample rate and channels, and a pointer to .jsonl files containing the prompt metadata and links to the corresponding audio files.\n",
        "- the above .jsonl file\n",
        "- a folder full of audio\n",
        "\n",
        "it looks for the .yaml at `content/audiocraft/config/dset/audio/YOUR_TRAINING_RUN.yaml`, which you point to by setting `dset=audio/YOUR_TRAINING_RUN` in the `dora run` command. In my case, `YOUR_TRAINING_RUN` is `train` in the example code.\n",
        "\n",
        "it looks like this:\n",
        "```\n",
        "datasource:\n",
        "  max_channels: 2\n",
        "  max_sample_rate: 44100\n",
        "\n",
        "  evaluate: egs/eval\n",
        "  generate: egs/train\n",
        "  train: egs/train\n",
        "  valid: egs/eval\n",
        "```\n",
        "you can use four different datasets, but it works just fine (if you're okay with overfitting!) with all of them pointing to the same one.\n",
        "\n",
        "The script is currently set up to put 15% of your dataset in `egs/eval` at random. This should help mitigate overfitting.\n",
        "\n",
        "- `train`: Data for training the model.\n",
        "- `valid`: Data for hyperparameter tuning and early stopping during training.\n",
        "- `evaluate`: Data for assessing the model's performance post-training.\n",
        "- `generate`: Data used for output generation, often the same as `train` but not necessarily.\n",
        "\n",
        "the `egs/YOUR_TRAINING_RUN` is referring to `content/audiocraft/egs/YOUR_TRAINING_RUN/data.jsonl`. the contents is a long list of lines that each look like this:\n",
        "\n",
        "```\n",
        "{\n",
        "  \"key\": \"\", \"artist\": \"\", \"sample_rate\": 44100, \"file_extension\": \"wav\",\n",
        "  \"description\": \"\", \"keywords\": \"\", \"duration\": 30.0, \"bpm\": \"\", \"genre\": \"\",\n",
        "  \"title\": \"\", \"name\": \"\", \"instrument\": \"\", \"moods\": [],\n",
        "  \"path\": os.path.join(dataset_folder, filename),\n",
        "}\n",
        "```\n",
        "any of these fields will be omitted if empty, only \"path\" is required.\n",
        "\n",
        "If you already have json tags for your dataset (eg: `filename1.wav` `filename1.json`, `filename2.wav` `filename2.json`), set `use_existing_json` to `True` in the following code. Otherwise, it will use a blank json template for all the files, as an example.\n",
        "\n",
        "## example:\n",
        "\n",
        "the below code is what I used to construct my dataset. it depends on a few assumptions about it, namely:\n",
        "- all the audios are 30 seconds long @ 44100 samples\n",
        "- they live in `content/drive/MyDrive/samples/train_musicgen_edm_uncond/train/outputs`\n",
        "- the name of my training run (\"YOUR_TRAINING_RUN\") is \"train\""
      ],
      "metadata": {
        "id": "yG68hiK5vxA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the .jsonl\n",
        "\n",
        "# if autolabelled:\n",
        "#     print('skip this cell, move on to the .yaml')\n",
        "#     exit(0)\n",
        "\n",
        "# if you have a .json file for each audio file sharing the same filename, set this variable to True\n",
        "use_existing_json = False\n",
        "\n",
        "# mount google drive to access dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import wave\n",
        "\n",
        "# make sure the .jsonl has a place to go\n",
        "os.makedirs(\"/content/audiocraft/egs/train\", exist_ok=True)\n",
        "os.makedirs(\"/content/audiocraft/egs/eval\", exist_ok=True)\n",
        "\n",
        "dataset_folder = \"/content/drive/MyDrive/samples/train_musicgen_edm_uncond/train/outputs\"\n",
        "train_manifest_path = \"/content/audiocraft/egs/train/data.jsonl\"\n",
        "eval_manifest_path = \"/content/audiocraft/egs/eval/data.jsonl\"\n",
        "\n",
        "dataset_len = 0\n",
        "train_len = 0\n",
        "eval_len = 0\n",
        "\n",
        "train_file = open(train_manifest_path, 'w')\n",
        "eval_file = open(eval_manifest_path, 'w')\n",
        "\n",
        "for filename in os.listdir(dataset_folder):\n",
        "    if filename.endswith(\".wav\"):\n",
        "        dataset_len += 1\n",
        "\n",
        "        if use_existing_json:\n",
        "            json_filepath = os.path.splitext(filename)[0] + \".json\"\n",
        "            if os.path.exists(json_filepath):\n",
        "                with open(json_filepath, 'r') as json_file:\n",
        "                    entry = json.load(json_file)\n",
        "            else:\n",
        "                print(f'error loading json: could not find {json_filepath}')\n",
        "        else:\n",
        "\n",
        "            # empty fields for now, alter as needed to match your metadata.\n",
        "            # all this does is make sure each file loads and trains semi-unconditionally\n",
        "            import librosa\n",
        "            y, sr = librosa.load(os.path.join(dataset_path, filename))\n",
        "            length = librosa.get_duration(y=y, sr=sr)\n",
        "\n",
        "            entry = {\n",
        "                \"key\": \"\",\n",
        "                \"artist\": \"\",\n",
        "                \"sample_rate\": 44100,\n",
        "                \"file_extension\": \"wav\",\n",
        "                \"description\": \"\",\n",
        "                \"keywords\": \"\",\n",
        "                \"duration\": length,\n",
        "                \"bpm\": \"\",\n",
        "                \"genre\": \"electronic\",\n",
        "                \"title\": \"\",\n",
        "                \"name\": \"\",\n",
        "                \"instrument\": \"\",\n",
        "                \"moods\": [],\n",
        "                \"path\": os.path.join(dataset_folder, filename),\n",
        "            }\n",
        "\n",
        "        if random.random() < 0.85:\n",
        "            train_len += 1\n",
        "            train_file.write(json.dumps(entry) + '\\n')\n",
        "        else:\n",
        "            eval_len += 1\n",
        "            eval_file.write(json.dumps(entry) + '\\n')\n",
        "\n",
        "train_file.close()\n",
        "eval_file.close()\n",
        "\n",
        "print(f'dataset length: {dataset_len} audio clips')\n",
        "print(f'train length: {train_len} audio clips')\n",
        "print(f'eval length: {eval_len} audio clips')"
      ],
      "metadata": {
        "id": "-2PGiow_wGZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the .yaml\n",
        "\n",
        "config_path = \"/content/audiocraft/config/dset/audio/train.yaml\"\n",
        "\n",
        "# point to the folders that your .jsonl is in\n",
        "data_path = \"egs/train\"\n",
        "eval_data_path = \"egs/eval\"\n",
        "\n",
        "package = \"package\" # yay python not letting me put #@.package in a string :/\n",
        "yaml_contents = f\"\"\"#@{package} __global__\n",
        "\n",
        "datasource:\n",
        "  max_channels: 2\n",
        "  max_sample_rate: 44100\n",
        "\n",
        "  evaluate: {eval_data_path}\n",
        "  generate: {data_path}\n",
        "  train: {data_path}\n",
        "  valid: {eval_data_path}\n",
        "\"\"\"\n",
        "\n",
        "with open(config_path, 'w') as yaml_file:\n",
        "    yaml_file.write(yaml_contents)"
      ],
      "metadata": {
        "id": "br7VuO_NvuwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training\n",
        "\n",
        "musicgen uses a thing called `dora` to launch the training run with a given solver, dataset, hyperparams, etc etc. if you've used anything like `accelerate`, it's like that. the command should be fairly easy to figure out from the given example.\n",
        "\n",
        "The below command is starting a finetuning run on the pretrained \"small\" model. Training from scratch is beyond the scope of this notebook, but shouldn't be too hard, just a *lot* of compute. Resuming from your finetuned chedkpoints will be covered later.\n",
        "\n",
        "Some info about VRAM requirements and training time:\n",
        "- It appears you can't train \"medium\" or \"large\" models on a single colab A100. They both OOM even with a batch size of 1. \"melody\" doesn't seem to load with anything i can pass to the `model/lm/model_scale=` param. You'll have to use \"small\"\n",
        "- here's the VRAM requirements with different batch sizes, using small model, 30s@44100 audios:\n",
        "\n",
        "```\n",
        "batch_size: VRAM\n",
        "1: 13.4 GB\n",
        "2: 14.6 GB\n",
        "4: 19.0 GB\n",
        "8: 27.5 GB\n",
        "12: 35.1 GB\n",
        "16: OOM\n",
        "```\n",
        "\n",
        "I did a training run with ~5000 of those samples, and it took roughly 90 minutes for 1 epoch to complete (on an A100). A checkpoint gets saved every epoch, more on this later. With a smaller dataset, you might have faster train times, I'm not sure (since the musicgen code says dataset size is disconnected from epoch length for this repo specifically, but they also say 1 epoch is roughly 30 minutes, so I can't take their word for it)\n",
        "\n",
        "You'll probably need quite a few epochs to get good results, I trained it for 4 epochs (about 6 hours) and the quality is still not great.\n",
        "\n",
        "NOTE: These numbers were from a previous test, before I updated the training parameters to be more efficient on a single GPU. There should be a ~50% speedup vs previous version. musicgen uses dadaw optimizer by default, which caches tokens, good for large runs on multiple gpus, but is less efficient on a single colab gpu.\n",
        "\n",
        "My example code stops after 5 epochs. change this number to suit your needs, I kept it here to mitigate losing my instance on crash. My personal version saves the `.th` to drive and resumes run, so if colab crashes while I'm afk, I can at most lose 5 epochs of compute.\n",
        "\n",
        "## example:"
      ],
      "metadata": {
        "id": "us8clCVnv1IY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env USER=lyra\n",
        "# CHANGE THIS\n",
        "\n",
        "command = (\n",
        "    \"dora run solver=musicgen/musicgen_base_32khz\"\n",
        "    \" model/lm/model_scale=small\"\n",
        "    \" continue_from=//pretrained/facebook/musicgen-small\"\n",
        "    \" conditioner=text2music\"\n",
        "    \" dset=audio/train\"\n",
        "    \" dataset.num_workers=2\"\n",
        "    \" dataset.valid.num_samples=1\"\n",
        "    \" dataset.batch_size=2\" # CHANGE THIS\n",
        "    \" schedule.cosine.warmup=8\"\n",
        "    \" optim.optimizer=adamw\" # uses dadaw by default, which is worse for single-gpu runs\n",
        "    \" optim.lr=1e-4\"\n",
        "    \" optim.epochs=5\" # stops training after 5 epochs- change this\n",
        "    \" optim.updates_per_epoch=1000\" # 2000 by default, change this if you want checkpoints quicker ig\n",
        "    \" optim.adam.weight_decay=0.01\"\n",
        ")\n",
        "\n",
        "!{command}"
      ],
      "metadata": {
        "id": "VcrtDHggv3qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d63ad3a-bcf1-4b52-f450-e4e27e417a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: USER=lyra\n",
            "2023-08-30 20:42:02.762635: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-30 20:42:03.670977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-30 20:42:05.140810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-08-30 20:42:05.141376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-08-30 20:42:05.141540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Dora directory: /tmp/audiocraft_lyra\n",
            "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  ret = run_job(\n",
            "[\u001b[36m08-30 20:42:10\u001b[0m][\u001b[34mdora.distrib\u001b[0m][\u001b[32mINFO\u001b[0m] - world_size is 1, skipping init.\u001b[0m\n",
            "[\u001b[36m08-30 20:42:10\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Instantiating solver MusicGenSolver for XP aec0903f\u001b[0m\n",
            "[\u001b[36m08-30 20:42:10\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - All XP logs are stored in /tmp/audiocraft_lyra/xps/aec0903f\u001b[0m\n",
            "[\u001b[36m08-30 20:42:10\u001b[0m][\u001b[34maudiocraft.solvers.builders\u001b[0m][\u001b[32mINFO\u001b[0m] - Loading audio data split evaluate: /content/audiocraft/egs/eval\u001b[0m\n",
            "[\u001b[36m08-30 20:42:10\u001b[0m][\u001b[34maudiocraft.solvers.builders\u001b[0m][\u001b[32mINFO\u001b[0m] - Loading audio data split generate: /content/audiocraft/egs/train\u001b[0m\n",
            "[\u001b[36m08-30 20:42:10\u001b[0m][\u001b[34maudiocraft.solvers.builders\u001b[0m][\u001b[32mINFO\u001b[0m] - Loading audio data split train: /content/audiocraft/egs/train\u001b[0m\n",
            "[\u001b[36m08-30 20:42:10\u001b[0m][\u001b[34maudiocraft.solvers.builders\u001b[0m][\u001b[32mINFO\u001b[0m] - Loading audio data split valid: /content/audiocraft/egs/eval\u001b[0m\n",
            "[\u001b[36m08-30 20:42:10\u001b[0m][\u001b[34mroot\u001b[0m][\u001b[32mINFO\u001b[0m] - Getting pretrained compression model from HF facebook/encodec_32khz\u001b[0m\n",
            "Downloading (…)lve/main/config.json: 100% 758/758 [00:00<00:00, 3.74MB/s]\n",
            "Downloading model.safetensors: 100% 236M/236M [00:00<00:00, 479MB/s]\n",
            "[\u001b[36m08-30 20:42:23\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Compression model has 4 codebooks with 2048 cardinality, and a framerate of 50\u001b[0m\n",
            "[\u001b[36m08-30 20:42:23\u001b[0m][\u001b[34maudiocraft.modules.conditioners\u001b[0m][\u001b[32mINFO\u001b[0m] - T5 will be evaluated with autocast as float32\u001b[0m\n",
            "Downloading (…)ve/main/spiece.model: 100% 792k/792k [00:00<00:00, 1.03MB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 1.21k/1.21k [00:00<00:00, 7.58MB/s]\n",
            "Downloading model.safetensors: 100% 892M/892M [00:02<00:00, 426MB/s]\n",
            "[\u001b[36m08-30 20:42:36\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Model hash: 18c0b8692a75ba87cc5c42546393c2b93898c452\u001b[0m\n",
            "[\u001b[36m08-30 20:42:36\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Initializing EMA on the model with decay = 0.99 every 10 updates\u001b[0m\n",
            "[\u001b[36m08-30 20:42:36\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Model size: 420.37 M params\u001b[0m\n",
            "[\u001b[36m08-30 20:42:36\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Base memory usage, with model, grad and optim: 6.73 GB\u001b[0m\n",
            "[\u001b[36m08-30 20:42:36\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Restoring weights and history.\u001b[0m\n",
            "[\u001b[36m08-30 20:42:36\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Loading a pretrained model. Ignoring 'load_best' and 'ignore_state_keys' params.\u001b[0m\n",
            "Downloading state_dict.bin: 100% 841M/841M [00:01<00:00, 483MB/s]\n",
            "[\u001b[36m08-30 20:42:38\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Checkpoint source is not the current xp: Load state_dict from best state.\u001b[0m\n",
            "[\u001b[36m08-30 20:42:38\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Ignoring keys when loading best []\u001b[0m\n",
            "[\u001b[36m08-30 20:42:38\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Loading state_dict from best state.\u001b[0m\n",
            "[\u001b[36m08-30 20:42:40\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Re-initializing EMA from best state\u001b[0m\n",
            "[\u001b[36m08-30 20:42:40\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Initializing EMA on the model with decay = 0.99 every 10 updates\u001b[0m\n",
            "[\u001b[36m08-30 20:42:43\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Model hash: 776d041cbbcb8973c4968782a79f9bb63b53a727\u001b[0m\n",
            "2023-08-30 20:42:47.016156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-30 20:42:53.756695: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m08-30 20:43:01\u001b[0m][\u001b[34maudiocraft.modules.codebooks_patterns\u001b[0m][\u001b[32mINFO\u001b[0m] - New pattern, time steps: 1500, sequence steps: 1504\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:762: UserWarning: Synchronization debug mode is a prototype feature and does not yet detect all synchronizing operations (Triggered internally at ../torch/csrc/cuda/Module.cpp:830.)\n",
            "  torch._C._cuda_set_sync_debug_mode(debug_mode)\n",
            "/content/audiocraft/audiocraft/solvers/musicgen.py:220: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:148.)\n",
            "  ce_targets = targets_k[mask_k]\n",
            "/content/audiocraft/audiocraft/solvers/musicgen.py:221: UserWarning: called a synchronizing CUDA operation (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:148.)\n",
            "  ce_logits = logits_k[mask_k]\n",
            "[\u001b[36m08-30 20:47:00\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 1 | 200/2000 | 0.83 it/sec | lr 9.77E-05 | grad_norm INF | grad_scale 17443.662 | ce 5.200 | ppl 202.915\u001b[0m\n",
            "[\u001b[36m08-30 20:51:00\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 1 | 400/2000 | 0.83 it/sec | lr 9.98E-05 | grad_norm 8.155E+00 | grad_scale 16384.000 | ce 5.076 | ppl 171.545\u001b[0m\n",
            "[\u001b[36m08-30 20:54:59\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 1 | 600/2000 | 0.83 it/sec | lr 9.94E-05 | grad_norm 7.148E+00 | grad_scale 16384.000 | ce 5.002 | ppl 161.459\u001b[0m\n",
            "[\u001b[36m08-30 20:58:59\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 1 | 800/2000 | 0.83 it/sec | lr 9.88E-05 | grad_norm 6.178E+00 | grad_scale 16384.000 | ce 4.956 | ppl 152.538\u001b[0m\n",
            "[\u001b[36m08-30 21:02:59\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 1 | 1000/2000 | 0.83 it/sec | lr 9.80E-05 | grad_norm 5.292E+00 | grad_scale 16384.000 | ce 4.930 | ppl 149.693\u001b[0m\n",
            "[\u001b[36m08-30 21:06:59\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 1 | 1200/2000 | 0.83 it/sec | lr 9.71E-05 | grad_norm 5.097E+00 | grad_scale 16384.000 | ce 4.915 | ppl 147.416\u001b[0m\n",
            "[\u001b[36m08-30 21:10:59\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 1 | 1400/2000 | 0.83 it/sec | lr 9.59E-05 | grad_norm 4.771E+00 | grad_scale 16384.000 | ce 4.788 | ppl 131.386\u001b[0m\n",
            "[\u001b[36m08-30 21:14:58\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 1 | 1600/2000 | 0.83 it/sec | lr 9.46E-05 | grad_norm 4.525E+00 | grad_scale 16384.000 | ce 4.778 | ppl 129.171\u001b[0m\n",
            "[\u001b[36m08-30 21:18:58\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 1 | 1800/2000 | 0.83 it/sec | lr 9.31E-05 | grad_norm 4.172E+00 | grad_scale 16384.000 | ce 4.829 | ppl 134.483\u001b[0m\n",
            "[\u001b[36m08-30 21:22:58\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[1mTrain Summary | Epoch 1 | lr=9.66E-05 | grad_norm=INF | grad_scale=16490.496 | ce=4.921 | ppl=150.350 | duration=2414.891\u001b[0m\n",
            "2023-08-30 21:23:02.049437: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-30 21:23:02.072417: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m08-30 21:23:06\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[1mValid Summary | Epoch 1 | ce=4.306 | ppl=74.117 | duration=7.704\u001b[0m\n",
            "[\u001b[36m08-30 21:23:06\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - New best state with ce=4.306 (was inf)\u001b[0m\n",
            "[\u001b[36m08-30 21:23:11\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Model hash: 68d3bc13637b11635cc59b3505a430e814741b28\u001b[0m\n",
            "[\u001b[36m08-30 21:23:33\u001b[0m][\u001b[34maudiocraft.utils.checkpoint\u001b[0m][\u001b[32mINFO\u001b[0m] - Checkpoint saved to /tmp/audiocraft_lyra/xps/aec0903f/checkpoint.th\u001b[0m\n",
            "2023-08-30 21:23:37.430840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-30 21:23:44.498315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m08-30 21:27:49\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 2 | 200/2000 | 0.83 it/sec | lr 8.96E-05 | grad_norm 4.173E+00 | grad_scale 32360.438 | ce 4.727 | ppl 123.097\u001b[0m\n",
            "[\u001b[36m08-30 21:31:50\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 2 | 400/2000 | 0.83 it/sec | lr 8.76E-05 | grad_norm 3.978E+00 | grad_scale 32768.000 | ce 4.703 | ppl 122.023\u001b[0m\n",
            "[\u001b[36m08-30 21:35:49\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 2 | 600/2000 | 0.83 it/sec | lr 8.54E-05 | grad_norm 3.477E+00 | grad_scale 32768.000 | ce 4.718 | ppl 121.525\u001b[0m\n",
            "[\u001b[36m08-30 21:39:49\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 2 | 800/2000 | 0.83 it/sec | lr 8.31E-05 | grad_norm 3.625E+00 | grad_scale 32768.000 | ce 4.673 | ppl 116.316\u001b[0m\n",
            "[\u001b[36m08-30 21:43:49\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 2 | 1000/2000 | 0.83 it/sec | lr 8.07E-05 | grad_norm 3.511E+00 | grad_scale 32768.000 | ce 4.681 | ppl 117.621\u001b[0m\n",
            "[\u001b[36m08-30 21:47:49\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 2 | 1200/2000 | 0.83 it/sec | lr 7.82E-05 | grad_norm 3.390E+00 | grad_scale 32768.000 | ce 4.591 | ppl 107.341\u001b[0m\n",
            "[\u001b[36m08-30 21:51:49\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 2 | 1400/2000 | 0.83 it/sec | lr 7.55E-05 | grad_norm 3.410E+00 | grad_scale 32768.000 | ce 4.564 | ppl 103.567\u001b[0m\n",
            "[\u001b[36m08-30 21:55:49\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 2 | 1600/2000 | 0.83 it/sec | lr 7.28E-05 | grad_norm 3.164E+00 | grad_scale 32768.000 | ce 4.583 | ppl 105.806\u001b[0m\n",
            "[\u001b[36m08-30 21:59:49\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Train | Epoch 2 | 1800/2000 | 0.83 it/sec | lr 6.99E-05 | grad_norm 3.127E+00 | grad_scale 32768.000 | ce 4.537 | ppl 100.208\u001b[0m\n",
            "[\u001b[36m08-30 22:03:48\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[1mTrain Summary | Epoch 2 | lr=7.90E-05 | grad_norm=3.490E+00 | grad_scale=32727.040 | ce=4.634 | ppl=112.235 | duration=2415.712\u001b[0m\n",
            "2023-08-30 22:03:53.420722: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-30 22:03:53.420726: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[36m08-30 22:03:58\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - \u001b[1mValid Summary | Epoch 2 | ce=4.214 | ppl=67.597 | duration=8.799\u001b[0m\n",
            "[\u001b[36m08-30 22:03:58\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - New best state with ce=4.214 (was 4.306)\u001b[0m\n",
            "[\u001b[36m08-30 22:04:03\u001b[0m][\u001b[34mflashy.solver\u001b[0m][\u001b[32mINFO\u001b[0m] - Model hash: c5b41dd66f58f80a9d0266b522fb9473ade93c1f\u001b[0m\n",
            "Error executing job with overrides: ['solver=musicgen/musicgen_base_32khz', 'model/lm/model_scale=small', 'continue_from=//pretrained/facebook/musicgen-small', 'conditioner=text2music', 'dset=audio/train', 'dataset.num_workers=2', 'dataset.valid.num_samples=1', 'dataset.batch_size=2', 'schedule.cosine.warmup=8', 'optim.optimizer=adamw', 'optim.lr=1e-4', 'optim.epochs=5', 'optim.adam.weight_decay=0.01']\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 441, in save\n",
            "    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 668, in _save\n",
            "    zip_file.write_record(name, storage.data_ptr(), num_bytes)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/audiocraft/audiocraft/train.py\", line 146, in main\n",
            "    return solver.run()\n",
            "  File \"/content/audiocraft/audiocraft/solvers/base.py\", line 499, in run\n",
            "    self.commit()\n",
            "  File \"/content/audiocraft/audiocraft/solvers/base.py\", line 461, in commit\n",
            "    self.save_checkpoints()\n",
            "  File \"/content/audiocraft/audiocraft/solvers/base.py\", line 306, in save_checkpoints\n",
            "    checkpoint.save_checkpoint(state, last_checkpoint_path, is_sharded)\n",
            "  File \"/content/audiocraft/audiocraft/utils/checkpoint.py\", line 100, in save_checkpoint\n",
            "    _safe_save_checkpoint(state, checkpoint_path, is_sharded)\n",
            "  File \"/content/audiocraft/audiocraft/utils/checkpoint.py\", line 154, in _safe_save_checkpoint\n",
            "    torch.save(state, f)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 440, in save\n",
            "    with _open_zipfile_writer(f) as opened_zipfile:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 305, in __exit__\n",
            "    self.file_like.write_end_of_file()\n",
            "RuntimeError: [enforce fail at inline_container.cc:337] . unexpected pos 1160032448 vs 1160032336\n",
            "\n",
            "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clear cuda mem\n",
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ],
      "metadata": {
        "id": "ebWYeRaJzVi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save, load, export\n",
        "\n",
        "Every epoch, the trainer saves a `checkpoint.th` file to `tmp/audiocraft_USER/xps/YOUR_RUN_SIG/checkpoint.th`.\n",
        "\n",
        "`USER` is set by `%env USER=lyra` from earlier, and `YOUR_RUN_SIG` should look something like \"2312e8a4\", and is found in the output of the training command, in a line that looks like this near the top:\n",
        "\n",
        "```\n",
        "[08-10 08:17:06][flashy.solver][INFO] - Instantiating solver MusicGenSolver for XP 2312e8a4\n",
        "[08-10 08:17:06][flashy.solver][INFO] - All XP logs are stored in /tmp/audiocraft_lyra/xps/2312e8a4\n",
        "```\n",
        "see https://github.com/facebookresearch/audiocraft/blob/main/docs/TRAINING.md#a-small-introduction-to-dora for more details on run signatures\n",
        "\n",
        "the `.th` file can't be loaded into musicgen for inference, but is required to resume a training run.\n",
        "\n",
        "to get a model that the generator can use, audiocraft comes with an export function. using it requires passing a model signature (unsure about passing a .th), so you'll have to do it in the same runtime as training.\n",
        "\n",
        "The export function makes two .bin files in the same folder: `state_dict.bin` and `compression_state_dict.bin`. the file sizes (for the small finetunes, at least) are ~800 MB and 1 KB respectively. These seem to load on top of the pretrained models, leading to lower file size than the original checkpoints. For reference, the small base model is ~2.5 GB, and the `checkpoint.th` is ~8.8 GB.\n",
        "\n",
        "loading the finetune involves passing the folder containing both `.bin` files to `MusicGen.get_pretrained()`, just like loading the base models.\n",
        "\n",
        "## the tricky parts\n",
        "\n",
        "colab's filesystem is temporary. this means if the runtime crashes before you've saved your `.th`, all that training is gone. the training code will continuously run, so there's no easy way to interrupt it to save the checkpoint before continuing. You'll need to monitor your training run, and make sure you have enough compute credits! it's always safer to stop, save, and resume than to hope it just keeps working.\n",
        "\n",
        "one way to try to get around this is exporting in a try/except block, so if it fails to load an audio file while you're away, at least you get the last checkpoint. (untested!) example at the end\n",
        "\n",
        "##examples:"
      ],
      "metadata": {
        "id": "9zlPbcLjv4qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exporting .bin files from a training run:\n",
        "\n",
        "from audiocraft.utils import export\n",
        "from audiocraft import train\n",
        "\n",
        "sig = \"aec0903f\"\n",
        "\n",
        "# from https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md#importing--exporting-models\n",
        "xp = train.main.get_xp_from_sig(sig)\n",
        "export.export_lm(xp.folder / 'checkpoint.th', '/content/checkpoints/finetune/state_dict.bin')\n",
        "export.export_pretrained_compression_model('facebook/encodec_32khz', '/content/checkpoints/finetune/compression_state_dict.bin')\n"
      ],
      "metadata": {
        "id": "uvtz2uKm59AR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b46a0477-46bf-4b3d-92ca-1f965a10a686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:audiocraft.environment:Dora directory: /tmp/audiocraft_lyra\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a finetune for inference:\n",
        "\n",
        "from audiocraft.models import MusicGen\n",
        "musicgen = MusicGen.get_pretrained('/content/checkpoints/finetune')"
      ],
      "metadata": {
        "id": "cOlUoUmh5_5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resuming a run:\n",
        "\n",
        "sig = \"aec0903f\"\n",
        "\n",
        "command = (\n",
        "    \"dora run solver=musicgen/musicgen_base_32khz\"\n",
        "    \" model/lm/model_scale=small\"\n",
        "\n",
        "    # you can continue a run this way, if the filesystem still exists:\n",
        "    f\" continue_from=//SIG/{sig}\"\n",
        "\n",
        "    # or you can save the .th file, load it in a new runtime, and resume from just it:\n",
        "    f\" continue_from=/tmp/audiocraft_lyra/xps/{sig}/checkpoint.th\"\n",
        "\n",
        "    \" conditioner=text2music\"\n",
        "    \" dset=audio/train\"\n",
        "    \" dataset.num_workers=2\"\n",
        "    \" dataset.valid.num_samples=1\"\n",
        "    \" dataset.batch_size=2\"\n",
        "    \" schedule.cosine.warmup=8\"\n",
        "    \" optim.optimizer=adamw\" # uses dadaw by default, which is worse for single-gpu runs\n",
        "    \" optim.lr=1e-4\"\n",
        "    \" optim.epochs=5\" # stops training after 5 epochs- change this\n",
        "    \" optim.adam.weight_decay=0.01\"\n",
        ")\n",
        "\n",
        "!{command}"
      ],
      "metadata": {
        "id": "3WkCSzDtv8ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the .th file to google drive for persistence:\n",
        "# it's ~8.8gb, so downloading from the colab filesystem is annoying. this is much easier\n",
        "# you can point directly to the checkpoint in google drive when resuming as well\n",
        "\n",
        "import shutil\n",
        "sig = \"aec0903f\"\n",
        "\n",
        "source_path = f'/tmp/audiocraft_lyra/xps/{sig}/checkpoint.th'\n",
        "destination_path = '/content/drive/MyDrive/musicgen_finetunes/checkpoints/new'\n",
        "os.makedirs(destination_path, exist_ok=True)\n",
        "shutil.copy(source_path, destination_path)"
      ],
      "metadata": {
        "id": "saZk_zzA5JVQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "06f161e0-be24-45fd-cdbc-4b42e1459769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/musicgen_finetunes/checkpoints/new/checkpoint.th'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Attempt to save checkpoint on crash!!\n",
        "sig = \"aec0903f\"\n",
        "\n",
        "try:\n",
        "    !{command}\n",
        "except:\n",
        "    import shutil\n",
        "    source_path = f'/tmp/audiocraft_lyra/xps/{sig}/checkpoint.th'\n",
        "    destination_path = '/content/drive/MyDrive/musicgen_finetunes/checkpoints/'\n",
        "    os.makedirs(destination_path, exist_ok=True)\n",
        "    shutil.copy(source_path, destination_path)"
      ],
      "metadata": {
        "id": "Xh61jbLQ7kcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# generate\n",
        "\n",
        "generating has been covered by many notebooks etc, but I'll include a few scripts here for different types of generating, including unconditional, text conditioned, sample continuation, pseudo stereo, multiband diffusion, and eventually more.\n",
        "\n",
        "note that melody is not included, since I cannot train the melody model with this script so it's beyond the scope of this notebook.\n",
        "\n",
        "for more info on using the multi-band diffusion model, read the docs here:\n",
        "https://github.com/facebookresearch/audiocraft/blob/main/docs/MBD.md\n",
        "\n",
        "Note: if you want to load the model from drive so you can use these examples without running the training script first, change `content/checkpoints/finetune` in the first cell to the path to the drive folder that your two `.bin` files are saved in. in the exporting example, it was `content/drive/MyDrive/musicgen_finetunes/checkpoints/`."
      ],
      "metadata": {
        "id": "UjevANVKv_dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from audiocraft.data.audio import audio_write\n",
        "import IPython.display as ipd\n",
        "from audiocraft.models import MusicGen\n",
        "import numpy as np\n",
        "\n",
        "# load your finetune\n",
        "musicgen = MusicGen.get_pretrained('/content/checkpoints/finetune')\n",
        "musicgen.set_generation_params(duration=16)"
      ],
      "metadata": {
        "id": "yLsA5LvCv-hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: unconditional generation\n",
        "\n",
        "wavs = musicgen.generate_unconditional(4)\n",
        "\n",
        "# save and display generated audio\n",
        "for idx, one_wav in enumerate(wavs):\n",
        "    audio_write(f'{idx}', one_wav.cpu(), musicgen.sample_rate, strategy=\"loudness\", loudness_compressor=True)\n",
        "    ipd.display(ipd.Audio(one_wav.cpu(), rate=32000))"
      ],
      "metadata": {
        "id": "zReKGkmKpAZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: text guided generation\n",
        "\n",
        "wavs = musicgen.generate([\n",
        "    'disco',\n",
        "    'slide guitar bluegrass',\n",
        "    'breakbeat, amen break',\n",
        "    'epic orchestral strings'\n",
        "])\n",
        "\n",
        "# save and display generated audio\n",
        "for idx, one_wav in enumerate(wavs):\n",
        "    audio_write(f'{idx}', one_wav.cpu(), musicgen.sample_rate, strategy=\"loudness\", loudness_compressor=True)\n",
        "    ipd.display(ipd.Audio(one_wav.cpu(), rate=32000))"
      ],
      "metadata": {
        "id": "-ZMKusYNpYzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions for handling sample input and continations\n",
        "# RUN THIS BEFORE RUNNING THE NEXT CELLS!\n",
        "\n",
        "import julius, torch\n",
        "\n",
        "def normalize_audio(audio_data):\n",
        "    max_value = torch.max(torch.abs(audio_data))\n",
        "    audio_data /= max_value\n",
        "    return audio_data\n",
        "\n",
        "def convert_audio_channels(wav: torch.Tensor, channels: int = 2) -> torch.Tensor:\n",
        "    *shape, src_channels, length = wav.shape\n",
        "    if src_channels == channels:\n",
        "        pass\n",
        "    elif channels == 1:\n",
        "        wav = wav.mean(dim=-2, keepdim=True)\n",
        "    elif src_channels == 1:\n",
        "        wav = wav.expand(*shape, channels, length)\n",
        "    elif src_channels >= channels:\n",
        "        wav = wav[..., :channels, :]\n",
        "    else:\n",
        "        raise ValueError('The audio file has less channels than requested but is not mono.')\n",
        "    return wav\n",
        "\n",
        "def convert_audio(wav: torch.Tensor, from_rate: float, to_rate: float, to_channels: int) -> torch.Tensor:\n",
        "    wav = julius.resample_frac(wav, int(from_rate), int(to_rate))\n",
        "    wav = convert_audio_channels(wav, to_channels)\n",
        "    return wav\n",
        "\n",
        "# runs musicgen.generate_continuation in 30s chunks and appends them until it reaches generation_length\n",
        "\n",
        "def generate_audio_continuation(musicgen, sample, generation_length, segment_length=30, overlap=10):\n",
        "    overlap_samples = overlap * 32000\n",
        "    segment_samples = segment_length * 32000\n",
        "    output = np.array([])\n",
        "    output = np.concatenate((output, sample.cpu().squeeze().numpy().astype(np.float32)))\n",
        "    init_length = len(output) / 32000\n",
        "\n",
        "    while len(output) / 32000 < generation_length:\n",
        "        musicgen.set_generation_params(duration=segment_length)\n",
        "        prompt = torch.tensor(np.array([output[-overlap_samples:]]), dtype=torch.float32)\n",
        "        res = musicgen.generate_continuation(prompt=prompt, prompt_sample_rate=32000)\n",
        "        res = res.cpu().squeeze().numpy().astype(np.float32)\n",
        "        output = np.concatenate((output, res[overlap_samples:]))\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "LVXEuvhnr-lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: sample continuation\n",
        "# run the helper functions cell or this won't work!\n",
        "\n",
        "# upload your file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "input_audio_filename = next(iter(uploaded.keys()))\n",
        "sample, sample_sr = torchaudio.load(input_audio_filename)\n",
        "sample = normalize_audio(sample)\n",
        "sample = convert_audio(sample, sample_sr, 32000, 1)\n",
        "\n",
        "# generate\n",
        "wav = generate_audio_continuation(musicgen, sample, 60)\n",
        "\n",
        "# save and display generated audio\n",
        "audio_write('continuation', output.cpu(), musicgen.sample_rate, strategy=\"loudness\", loudness_compressor=True)\n",
        "ipd.display(ipd.Audio(output, rate=32000))"
      ],
      "metadata": {
        "id": "O-urbQ1rp0zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: long generations (self-continuation)\n",
        "# run the helper functions cell or this won't work!\n",
        "\n",
        "# this is unconditional for the example, swap in text guidance as needed.\n",
        "wavs = musicgen.generate_unconditional(4)\n",
        "\n",
        "# continuations only work on one sample at a time\n",
        "for idx, wav in enumerate(wavs):\n",
        "\n",
        "    wav = generate_audio_continuation(musicgen, wav, 60)\n",
        "\n",
        "    audio_write(f'{idx}', wav.cpu(), musicgen.sample_rate, strategy=\"loudness\", loudness_compressor=True)\n",
        "    ipd.display(ipd.Audio(wav.cpu(), rate=32000))"
      ],
      "metadata": {
        "id": "ABzE8Jyyr0Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6: multiband diffusion decoder\n",
        "\n",
        "from audiocraft.models import MusicGen, MultiBandDiffusion\n",
        "mbd = MultiBandDiffusion.get_mbd_musicgen()\n",
        "\n",
        "# use mbd to generate the audio from the codebook tokens\n",
        "wavs, tokens = musicgen.generate_unconditional(4, return_tokens=True)\n",
        "wavs_diffusion = mbd.tokens_to_wav(tokens)\n",
        "\n",
        "# save and display generated audio\n",
        "for idx, one_wav in enumerate(wavs):\n",
        "    audio_write(f'{idx}', one_wav.cpu(), musicgen.sample_rate, strategy=\"loudness\", loudness_compressor=True)\n",
        "    audio_write(f'{idx}_diffusion', wavs_diffusion[idx].cpu(), musicgen.sample_rate, strategy=\"loudness\", loudness_compressor=True)\n",
        "\n",
        "    print('default decoder:')\n",
        "    ipd.display(ipd.Audio(one_wav.cpu(), rate=32000))\n",
        "    print('multiband diffusion:')\n",
        "    ipd.display(ipd.Audio(wavs_diffusion[idx].cpu().cpu(), rate=32000))"
      ],
      "metadata": {
        "id": "X3L0vDwKvm6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# local installation for inference\n",
        "\n",
        "this section assumes you have a GPU with 8gb of vram. it only covers inference, since you cannot train on only 8gb vram. if you have the gpu spec to train, this should get you started but you may have to figure out the details yourself (i cannot test it, my gpu is 8gb). I'll assume you have python installed already.\n",
        "\n",
        "## step 1 - environment\n",
        "\n",
        "I use miniconda for managing environments for each project. you can download it here: https://docs.conda.io/en/main/miniconda.html\n",
        "\n",
        "once it's installed, run:\n",
        "\n",
        "```\n",
        "conda create --name musicgen\n",
        "conda activate musicgen\n",
        "```\n",
        "\n",
        "make a folder for your musicgen projects to live. you'll want to `cd` to this folder before running the commands in the next step.\n",
        "\n",
        "## step 2 - audiocraft\n",
        "\n",
        "now you're in your environment and can start installing.\n",
        "\n",
        "I'm going to assume you have git installed, if not, https://gitforwindows.org/ (other platforms- find your own. it won't be hard).\n",
        "\n",
        "You should recognize these commands from the top of this notebook:\n",
        "\n",
        "```\n",
        "git clone https://github.com/facebookresearch/audiocraft.git\n",
        "cd audiocraft\n",
        "pip install -e .\n",
        "pip install dora-search julius\n",
        "```\n",
        "\n",
        "running these out of the box should install a bunch of dependencies, but it'll give you a pytorch-cpu version with no GPU access by default. You'll need a CUDA version to actually run this.\n",
        "\n",
        "## step 3 - torch with cuda\n",
        "\n",
        "You will need cuda for this to work. if you don't have it, go get it here: https://developer.nvidia.com/cuda-downloads\n",
        "\n",
        "`pip install --upgrade torch --extra-index-url https://download.pytorch.org/whl/cu117`\n",
        "\n",
        "You can't install torch with cuda *first*, because audiocraft will overwrite it during install.\n",
        "\n",
        "## step 4 - make a script and run\n",
        "\n",
        "at this point, everything is installed and you can do whatever with the environment. here's an example script you can use to make sure everything is loaded and working properly:\n",
        "\n",
        "```\n",
        "import torchaudio\n",
        "from audiocraft.models import MusicGen\n",
        "from audiocraft.data.audio import audio_write\n",
        "\n",
        "print('loading model...')\n",
        "model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
        "model.set_generation_params(duration=8)\n",
        "\n",
        "print('generating...')\n",
        "wav = model.generate_unconditional(4)\n",
        "\n",
        "print('saving...')\n",
        "for idx, one_wav in enumerate(wav):\n",
        "    audio_write(f'{idx}', one_wav.cpu(), model.sample_rate, strategy=\"loudness\", loudness_compressor=True)\n",
        "\n",
        "print('done!')\n",
        "```\n",
        "\n",
        "save this as `example.py` and run it, and you should get a `1.wav` file after a little bit. I won't go into too much detail since I figure you know how to run a python script.\n",
        "\n",
        "if you want to use your finetunes, go ahead and make a folder for them to live (perhaps `checkpoints/finetune_1`, `checkpoints/finetune_2`, etc), and use the examples given above to load them from your folder.\n",
        "\n",
        "If you're trying to train this with `dora` on Windows, you'll need to make some modifications:\n",
        "\n",
        "\n",
        "> in `audiocraft/utils/cluster.py`: comment out lines 28-40\n",
        "\n",
        "> in `audiocraft/train.py`: comment out line 110\n",
        "\n",
        "> in the above code:\n",
        "\n",
        "```\n",
        "command = (\n",
        "    \"dora -P audiocraft run\"\n",
        "    ...\n",
        "```\n",
        "Note that the above command is expecting to be executed from the audiocraft directory, looking back one folder: `C:/...project_folder/audiocraft>python ../train_musicgen.py` where `train_musicgen.py` is in `project_folder` and contains the above code.\n"
      ],
      "metadata": {
        "id": "RUjdT47y0hJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# multi-gpu training on runpod\n",
        "\n",
        "TBA, see musicgen discord for current progress. documentation soon!\n",
        "\n",
        "notes:\n",
        "\n",
        "- run command needs updating with `-d` flag (`\"dora -P audiocraft run -d \"`) and `\" fsdp.use=true\"` and `\" autocast=false\"`\n",
        "\n",
        "- fsdp allows more efficient memory usage so any size model can be trained with 2xA6000s (48gb combined vram). further documentation assumes this setup. the cost is $1.58/hr for 2xA6000\n",
        "\n",
        "- if training `large` model, `batch_size=4`. if training `medium`, `batch_size=16`\n",
        "\n",
        "- runpod used /workspace/ everywhere colab would usually use /content/. something you might want to know for making a notebook\n",
        "\n",
        "- do scraping and labelling first (locally or colab), bc you dont really need a lot of GPU for that. put the data somewhere downloadable (i have a 13gb .tar.gz in google cloud storage, alongside the `train.jsonl` `test.jsonl` files, all publicly accessible so i can just fetch them from the instance)\n",
        "\n",
        "- it will die on eval after 1 epoch. to get rid of the deadlock, comment out lines 478-487 in `audiocraft/audiocraft/solvers/base.py`\n",
        "\n",
        "- it will die on checkpoint save if you don't give the instance enough disk space. your dataset should be in a network drive, to leave disk space for the checkpoints. `small` model training checkpoint is 8gb, `medium` is 30gb, and `large` is 68gb. it also uses 2x storage because it writes a temp checkpoint before replacing, so give yourself like 256gb disk if you're trying to train large model. 128 works for medium. you'll need to do this during instance config before its actually launched. it's called \"advanced settings\" or something in the first screen."
      ],
      "metadata": {
        "id": "aP0o4lm-9L70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training from scratch with DAC\n",
        "\n",
        "TBA, still being testing. Below code may help you get started but probably won't figure everything out for you:\n",
        "\n",
        "```py\n",
        "!git clone https://github.com/facebookresearch/audiocraft.git\n",
        "%cd audiocraft\n",
        "!pip install -e .\n",
        "!pip install dora-search descript-audio-codec\n",
        "!pip install -U protobuf\n",
        "```\n",
        "\n",
        "```py\n",
        "%env USER=lyra\n",
        "command = (\n",
        "    \"dora -P audiocraft run -d\"\n",
        "    \" solver=musicgen/musicgen_base_32khz\"\n",
        "        # should probably put all this in a config, its no longer 32k\n",
        "        # mostly just using this to handle defaults i dont want to worry about setting here\n",
        "    \" model/lm/model_scale=large\"\n",
        "    \" compression_model_checkpoint=//pretrained/dac_44khz\"\n",
        "    \" sample_rate=44100\"\n",
        "    \" transformer_lm.card=1024\"\n",
        "        # unsure purpose, required to pass asserts\n",
        "    \" transformer_lm.n_q=9\"\n",
        "    \" codebooks_pattern.modeling=delay\"\n",
        "        # higher speed, lower quality - not optimal.\n",
        "        # see audiocraft/audiocraft/modules/codebooks_patterns.py for more\n",
        "    \" codebooks_pattern.delay.delays=[0,1,2,3,4,5,6,7,8]\"\n",
        "        # only required if using delay pattern above, other options untested\n",
        "    \" conditioner=text2music\"\n",
        "    \" dset=audio/train\"\n",
        "    \" dataset.num_workers=2\"\n",
        "    \" dataset.valid.num_samples=1\"\n",
        "    \" dataset.batch_size=2\"\n",
        "    \" schedule.cosine.warmup=8\"\n",
        "    \" optim.optimizer=adamw\"\n",
        "    \" optim.lr=1e-4\"\n",
        "    \" optim.epochs=5\"\n",
        "    \" optim.updates_per_epoch=10\"\n",
        "    \" optim.adam.weight_decay=0.01\"\n",
        "    \" fsdp.use=true\"\n",
        "    \" autocast=false\"\n",
        ")\n",
        "!{command}\n",
        "```"
      ],
      "metadata": {
        "id": "uBGn9yn1F352"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DISCLAIMER: Not everything in this notebook is guaranteed to run first try. I do my best to keep it functional, but edge cases and bugs slip thru my fingers frequently. If something is broken or needs changing, please let me know!\n",
        "\n",
        "I'm `lyraaaa_` on discord, and [@bleepybloops](https://twitter.com/bleepybloops) on twitter/X.\n",
        "\n",
        "Community musicgen discord server for development/testing, sharing outputs, and asking questions:\n",
        "\n",
        "https://discord.gg/h498MAYZ4e"
      ],
      "metadata": {
        "id": "T2NUtvVVIxtD"
      }
    }
  ]
}